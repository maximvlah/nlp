{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import gensim\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "source": [
    "## Amazon Fine Food Reviews Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df = pd.read_csv('data/amazon_food_reviews/Reviews.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 summary                                               text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#select only the columns we need\n",
    "df = df[['Summary','Text']]\n",
    "#rename columns\n",
    "df.columns = ['summary','text']\n",
    "#drop missing values\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  summary  \\\n",
       "0                       Worst K-Cup ever!   \n",
       "1                    Scrumdiddiliumptious   \n",
       "2        tasty product for parents & kids   \n",
       "3                             Jennifer K.   \n",
       "4               My son loves this cereal!   \n",
       "...                                   ...   \n",
       "9995   Great mixed with flavored coffees!   \n",
       "9996  No more tummy troubles for my kitty   \n",
       "9997                          Good coffee   \n",
       "9998                 Very tasty & healthy   \n",
       "9999                          Good Coffee   \n",
       "\n",
       "                                                   text  \n",
       "0     This is by far, the worst tasting K-Cup variet...  \n",
       "1     These are the best coffee beans I've ever tast...  \n",
       "2     The only reason I gave this a 4 star was that ...  \n",
       "3     My vet recommended these chewies because i'm s...  \n",
       "4     No joke--this was the first solid food we gave...  \n",
       "...                                                 ...  \n",
       "9995  At first glance, Timothy's White Hot Chocolate...  \n",
       "9996  I have a cat with tummy/bowel problems but not...  \n",
       "9997  Just finished off my first Amazon packaged (50...  \n",
       "9998  I first discovered these crackers when looking...  \n",
       "9999  Good Coffee and 50 pack is extremely convenien...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Worst K-Cup ever!</td>\n      <td>This is by far, the worst tasting K-Cup variet...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Scrumdiddiliumptious</td>\n      <td>These are the best coffee beans I've ever tast...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tasty product for parents &amp; kids</td>\n      <td>The only reason I gave this a 4 star was that ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jennifer K.</td>\n      <td>My vet recommended these chewies because i'm s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>My son loves this cereal!</td>\n      <td>No joke--this was the first solid food we gave...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>Great mixed with flavored coffees!</td>\n      <td>At first glance, Timothy's White Hot Chocolate...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>No more tummy troubles for my kitty</td>\n      <td>I have a cat with tummy/bowel problems but not...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>Good coffee</td>\n      <td>Just finished off my first Amazon packaged (50...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>Very tasty &amp; healthy</td>\n      <td>I first discovered these crackers when looking...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>Good Coffee</td>\n      <td>Good Coffee and 50 pack is extremely convenien...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# select small portion of reviews to test how algorithm is working\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df[:10000]\n",
    "df"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords with spacy\n",
    "def remove_stopwords(nlp, text):\n",
    "    #tokenize\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in doc:\n",
    "        token_list.append(token.text)\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence = [] \n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# lemmatize text with spacy\n",
    "def lemmatize(nlp,text):\n",
    "\n",
    "    #tokenize\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lemmatized = []\n",
    "    for token in doc:\n",
    "        # print(token, token.lemma, token.lemma_)\n",
    "        lemmatized.append(token.lemma_)\n",
    "\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "#Clean text\n",
    "def clean_text(nlp, text):\n",
    "    \n",
    "    clean = text\n",
    "    #remove links\n",
    "    clean = re.sub(r\"http\\S+\", \"\", clean)\n",
    "    #remove punctuation\n",
    "    clean = clean.translate(str.maketrans('', '', string.punctuation)) #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "    #remove stopwords\n",
    "    clean = remove_stopwords(nlp, clean)\n",
    "    #lemmatizing\n",
    "    clean = lemmatize(nlp, clean)\n",
    "    #strip whitespaces\n",
    "    clean = clean.strip()\n",
    "    #lowercase\n",
    "    clean = clean.lower()\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10000/10000 [04:36<00:00, 36.16it/s]\n"
     ]
    }
   ],
   "source": [
    "PREPROCESS = True\n",
    "\n",
    "#Pre-Process\n",
    "if PREPROCESS:\n",
    "    #Load spacy model -> needed for stopword removal and lemmatizing\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #Enable progress tracking\n",
    "    tqdm.pandas()\n",
    "    #Run pre-processing on the whole dataset\n",
    "    df['clean'] = df.text.progress_apply(lambda x: clean_text(nlp,x))\n",
    "    #Save pre-processed data\n",
    "    df.to_csv('data/amazon_food_reviews/cleaned_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            summary  \\\n",
       "0                 Worst K-Cup ever!   \n",
       "1              Scrumdiddiliumptious   \n",
       "2  tasty product for parents & kids   \n",
       "3                       Jennifer K.   \n",
       "4         My son loves this cereal!   \n",
       "\n",
       "                                                text  \\\n",
       "0  This is by far, the worst tasting K-Cup variet...   \n",
       "1  These are the best coffee beans I've ever tast...   \n",
       "2  The only reason I gave this a 4 star was that ...   \n",
       "3  My vet recommended these chewies because i'm s...   \n",
       "4  No joke--this was the first solid food we gave...   \n",
       "\n",
       "                                               clean  \n",
       "0  far bad tasting kcup variety try date luckily ...  \n",
       "1  good coffee bean ve tasted life careful eat night  \n",
       "2  reason give 4 star bulk pack thing    geared i...  \n",
       "3  vet recommend chewie m awful brushing dog toot...  \n",
       "4  jokethis solid food give son go hmm hmm yum yu...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>text</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Worst K-Cup ever!</td>\n      <td>This is by far, the worst tasting K-Cup variet...</td>\n      <td>far bad tasting kcup variety try date luckily ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Scrumdiddiliumptious</td>\n      <td>These are the best coffee beans I've ever tast...</td>\n      <td>good coffee bean ve tasted life careful eat night</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tasty product for parents &amp; kids</td>\n      <td>The only reason I gave this a 4 star was that ...</td>\n      <td>reason give 4 star bulk pack thing    geared i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jennifer K.</td>\n      <td>My vet recommended these chewies because i'm s...</td>\n      <td>vet recommend chewie m awful brushing dog toot...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>My son loves this cereal!</td>\n      <td>No joke--this was the first solid food we gave...</td>\n      <td>jokethis solid food give son go hmm hmm yum yu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(df[\"clean\"])"
   ]
  },
  {
   "source": [
    "## LDA Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration: 1 of max_iter: 20\n",
      "iteration: 2 of max_iter: 20\n",
      "iteration: 3 of max_iter: 20\n",
      "iteration: 4 of max_iter: 20\n",
      "iteration: 5 of max_iter: 20\n",
      "iteration: 6 of max_iter: 20\n",
      "iteration: 7 of max_iter: 20\n",
      "iteration: 8 of max_iter: 20\n",
      "iteration: 9 of max_iter: 20\n",
      "iteration: 10 of max_iter: 20\n",
      "iteration: 11 of max_iter: 20\n",
      "iteration: 12 of max_iter: 20\n",
      "iteration: 13 of max_iter: 20\n",
      "iteration: 14 of max_iter: 20\n",
      "iteration: 15 of max_iter: 20\n",
      "iteration: 16 of max_iter: 20\n",
      "iteration: 17 of max_iter: 20\n",
      "iteration: 18 of max_iter: 20\n",
      "iteration: 19 of max_iter: 20\n",
      "iteration: 20 of max_iter: 20\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation Model\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=20, learning_method='online',verbose=True)\n",
    "data_lda = lda.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "source": [
    "## Inspect LDA Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]) \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic 0:\n[('store', 831.010570358646), ('local', 509.7418145372918), ('product', 504.73763374478074), ('buy', 406.9158424116845), ('grocery', 395.2403264183786), ('price', 333.994721748483), ('amazon', 324.6292303286209), ('year', 293.8964448076468), ('bottle', 276.38340038966913), ('brand', 273.428664296629)]\n\nTopic 1:\n[('bag', 1028.3412593229943), ('buy', 344.62055974940904), ('chip', 300.008460236328), ('candy', 279.8525170597371), ('store', 195.06798245195745), ('price', 174.0197197463074), ('piece', 159.16246681087588), ('thank', 146.208319321236), ('good', 140.33829903697324), ('amazon', 128.16059917130877)]\n\nTopic 2:\n[('tea', 2563.1625430648187), ('drink', 1100.4932479081397), ('taste', 924.3614035480563), ('like', 846.3469375962175), ('flavor', 620.8473881638367), ('green', 520.0564338764792), ('water', 374.306991096445), ('good', 308.8207037536486), ('try', 289.0831005060041), ('ice', 255.20175782889837)]\n\nTopic 3:\n[('taste', 1746.7077016867104), ('like', 1415.8329335140531), ('flavor', 1362.2861830134373), ('good', 1174.9558058254488), ('chocolate', 937.5089456722567), ('great', 868.6565348373157), ('try', 776.8853791356036), ('sugar', 668.5168815697155), ('salt', 618.2402753135808), ('eat', 577.2988918431126)]\n\nTopic 4:\n[('food', 2025.6573884565732), ('dog', 1654.721189966337), ('cat', 1068.3044026432804), ('treat', 1050.1379137323268), ('love', 1006.8892720039375), ('eat', 954.1819674820703), ('like', 644.9096052661005), ('old', 429.2436364294098), ('try', 423.0175402840904), ('chew', 343.2604233389914)]\n\nTopic 5:\n[('water', 872.077499145715), ('use', 561.0462850829846), ('sauce', 479.6364461907464), ('add', 458.6057114168914), ('easy', 329.93789379240036), ('product', 305.5083322531637), ('cook', 298.2728767447201), ('hot', 289.1389407767389), ('soup', 281.12651122048067), ('try', 272.16015481727203)]\n\nTopic 6:\n[('coffee', 3030.7066080711134), ('flavor', 1444.348708698066), ('cup', 1035.8033775934039), ('good', 1030.6072709537061), ('like', 1009.4623485931311), ('taste', 901.8683581676768), ('try', 780.6059396548901), ('strong', 522.3474641756144), ('great', 435.3345346382046), ('love', 400.1954312037561)]\n\nTopic 7:\n[('product', 1388.3296004150595), ('order', 1273.5658803006268), ('buy', 1062.5693362894767), ('box', 1016.7243861980724), ('great', 982.010169790239), ('amazon', 905.829416746246), ('good', 801.138695013771), ('price', 765.7844832450243), ('time', 735.1179451392201), ('purchase', 710.8910835027139)]\n\nTopic 8:\n[('normally', 150.5644525562248), ('say', 130.9707493935711), ('cause', 119.61403629796303), ('blood', 109.3488967915287), ('new', 106.75985012373283), ('number', 102.49825386308532), ('change', 98.93215267096244), ('plant', 98.56170786611507), ('matcha', 98.06268829460318), ('red', 85.86976892702853)]\n\nTopic 9:\n[('bar', 780.2382000440574), ('cookie', 755.9521742254312), ('eat', 458.34993464519096), ('snack', 457.2272081573758), ('kid', 362.2354180908556), ('good', 321.4065148836131), ('love', 282.234580549248), ('organic', 255.60224893033535), ('syrup', 227.6287902678357), ('sweet', 215.1825791966299)]\n\n"
     ]
    }
   ],
   "source": [
    "selected_topics(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}